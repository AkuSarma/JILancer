{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1967a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828ad235",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = pd.read_csv(\"Combined_Jobs_Final.csv\")\n",
    "df = pd.read_csv(\"Combined_Jobs_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4e1456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job.ID</th>\n",
       "      <th>Provider</th>\n",
       "      <th>Status</th>\n",
       "      <th>Slug</th>\n",
       "      <th>Title</th>\n",
       "      <th>Position</th>\n",
       "      <th>Company</th>\n",
       "      <th>City</th>\n",
       "      <th>State.Name</th>\n",
       "      <th>State.Code</th>\n",
       "      <th>...</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Job.Description</th>\n",
       "      <th>Requirements</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Listing.Start</th>\n",
       "      <th>Listing.End</th>\n",
       "      <th>Employment.Type</th>\n",
       "      <th>Education.Required</th>\n",
       "      <th>Created.At</th>\n",
       "      <th>Updated.At</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>palo-alto-ca-tacolicious-server</td>\n",
       "      <td>Server @ Tacolicious</td>\n",
       "      <td>Server</td>\n",
       "      <td>Tacolicious</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>Food and Beverages</td>\n",
       "      <td>Tacolicious' first Palo Alto store just opened...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Part-Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-03-12 02:08:28 UTC</td>\n",
       "      <td>2014-08-16 15:35:36 UTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>113</td>\n",
       "      <td>1</td>\n",
       "      <td>open</td>\n",
       "      <td>san-francisco-ca-claude-lane-kitchen-staff-chef</td>\n",
       "      <td>Kitchen Staff/Chef @ Claude Lane</td>\n",
       "      <td>Kitchen Staff/Chef</td>\n",
       "      <td>Claude Lane</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>California</td>\n",
       "      <td>CA</td>\n",
       "      <td>...</td>\n",
       "      <td>Food and Beverages</td>\n",
       "      <td>\\r\\n\\r\\nNew French Brasserie in S.F. Financia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Part-Time</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-04-12 08:36:36 UTC</td>\n",
       "      <td>2014-08-16 15:35:36 UTC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Job.ID  Provider Status                                             Slug  \\\n",
       "0     111         1   open                  palo-alto-ca-tacolicious-server   \n",
       "1     113         1   open  san-francisco-ca-claude-lane-kitchen-staff-chef   \n",
       "\n",
       "                              Title            Position      Company  \\\n",
       "0              Server @ Tacolicious              Server  Tacolicious   \n",
       "1  Kitchen Staff/Chef @ Claude Lane  Kitchen Staff/Chef  Claude Lane   \n",
       "\n",
       "            City  State.Name State.Code  ...            Industry  \\\n",
       "0      Palo Alto  California         CA  ...  Food and Beverages   \n",
       "1  San Francisco  California         CA  ...  Food and Beverages   \n",
       "\n",
       "                                     Job.Description  Requirements Salary  \\\n",
       "0  Tacolicious' first Palo Alto store just opened...           NaN    8.0   \n",
       "1   \\r\\n\\r\\nNew French Brasserie in S.F. Financia...           NaN    0.0   \n",
       "\n",
       "  Listing.Start  Listing.End  Employment.Type Education.Required  \\\n",
       "0           NaN          NaN        Part-Time                NaN   \n",
       "1           NaN          NaN        Part-Time                NaN   \n",
       "\n",
       "                Created.At               Updated.At  \n",
       "0  2013-03-12 02:08:28 UTC  2014-08-16 15:35:36 UTC  \n",
       "1  2013-04-12 08:36:36 UTC  2014-08-16 15:35:36 UTC  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e46f57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84090\n",
      "84090\n"
     ]
    }
   ],
   "source": [
    "print(len(job_df))\n",
    "job_df = job_df[['Status', 'Title', 'Position', 'Company', 'Job.Description']]\n",
    "print(len(job_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d67d44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84090, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36c6dbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hiring Event Details\\r\\nStore Associate\\r\\n\\r\\n$12.00 / Hour\\r\\nAdditional $1.00 Per Hour For ALL Sunday Shifts!\\r\\n50 Cent Wage Increases Beginning At 6 Months - Up to $13.50 At 2 Years\\r\\n\\r\\nMonday, December 15, 2014\\r\\n9am - 11am\\r\\n\\r\\nALDI\\r\\n3133 Market Place Dr\\r\\nOnalaska, WI 54650\\r\\n\\r\\n&nbsp;\\r\\nFor consideration, please apply in person at the hiring event only. Get started now by downloading our Store Employment Application.\\r\\n\\r\\nStore Associate - Retail Sales ( Customer Service )\\r\\n\\r\\nIf you are a customer service minded individual with a positive and energetic personality and you&rsquo;re interested in working for one of the best-known grocery stores in the nation, join the ALDI family! We are looking for motivated and reliable individuals to serve as a Store Associate. You will serve as the face of ALDI, providing customers with friendly and efficient check-out services. But that&rsquo;s just the beginning. You will also assist the store manager in a variety of roles, from stocking and merchandising our products to monitoring inventory and keeping the store looking clean and inviting. This is also an excellent ground-floor opportunity for you if you are interested in pursuing a management career, as we prefer to promote from within whenever possible. If you are a people person who likes to roll up your sleeves and put in a good day&rsquo;s work, we want to talk with you!\\r\\n\\r\\n\\r\\nStore Associate - Retail Sales ( Customer Service )\\r\\n\\r\\nJob Responsibilities\\r\\n&nbsp;\\r\\nAs a Store Associate, you will be involved in all aspects of keeping the store looking and functioning at its best. First and foremost, of course, you will keep your checkout line moving as quickly and smoothly as possible while ensuring that customers have a pleasant and positive shopping experience. In addition, you will have a variety of other duties throughout the store, which you will perform on an as-needed basis.\\r\\n&nbsp;\\r\\nYour specific duties as a Store Associate will include:\\r\\n&nbsp;\\r\\n\\r\\n    \\r\\n    Providing friendly and informative customer service\\r\\n    \\r\\n    Ringing up customers quickly, efficiently, and with a smile\\r\\n    \\r\\n    Maintaining professional appearance and demeanor at all times\\r\\n    \\r\\n    Making a positive impression on customers to encourage word-of-mouth referrals\\r\\n    \\r\\n    Scanning products\\r\\n    \\r\\n    Conducting cash and inventory control\\r\\n    \\r\\n    Maintaining displays and ensuring that they are kept stocked and up to company standards\\r\\n    \\r\\n    Loading and unloading delivery trucks\\r\\n    \\r\\n    Rotating stock\\r\\n    \\r\\n    Keeping the store clean (floors, registers, bathrooms, etc.)\\r\\n\\r\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df['Job.Description'][20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48f6d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status             0\n",
       "Title              0\n",
       "Position           0\n",
       "Company            0\n",
       "Job.Description    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df.isnull().sum()\n",
    "job_df.fillna('',inplace=True)\n",
    "job_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035ec4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = job_df.sample(n=1000,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baddc2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54970a8b",
   "metadata": {},
   "source": [
    "# cleaning dataset\n",
    "keeping all letters and digits                          \n",
    "lover case                             \n",
    "removing stopwords                            \n",
    "tokenization                            \n",
    "stemming                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4049f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "856403b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(txt):\n",
    "#     step 1\n",
    "    txt = re.sub(r'[^a-zA-Z0-9\\s]','',txt)\n",
    "#     step 2\n",
    "    tokens = nltk.word_tokenize(txt.lower())\n",
    "    # step 3 and 5\n",
    "    stemming = [ps.stem(w) for w in tokens if w not in stopwords.words('english')]\n",
    "    return \" \".join(stemming)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b93699d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\priet/nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cleaning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124mhelo the master piece is my loving moving cat @9032#\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mcleaning\u001b[1;34m(txt)\u001b[0m\n\u001b[0;32m      3\u001b[0m     txt \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z0-9\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,txt)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     step 2\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(txt\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# step 3 and 5\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     stemming \u001b[38;5;241m=\u001b[39m [ps\u001b[38;5;241m.\u001b[39mstem(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\priet/nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "cleaning(\"\\n\\rhelo the master piece is my loving moving cat @9032#%$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf3c9491",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\priet/nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob.Description\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob.Description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: cleaning(x))\n\u001b[0;32m      2\u001b[0m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: cleaning(x))\n\u001b[0;32m      3\u001b[0m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: cleaning(x))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\u001b[38;5;28mself\u001b[39m, func, convert_dtype, args, kwargs)\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmap_infer(\n\u001b[0;32m   1077\u001b[0m             values,\n\u001b[0;32m   1078\u001b[0m             f,\n\u001b[0;32m   1079\u001b[0m             convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype,\n\u001b[0;32m   1080\u001b[0m         )\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob.Description\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJob.Description\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: cleaning(x))\n\u001b[0;32m      2\u001b[0m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: cleaning(x))\n\u001b[0;32m      3\u001b[0m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m job_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPosition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: cleaning(x))\n",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m, in \u001b[0;36mcleaning\u001b[1;34m(txt)\u001b[0m\n\u001b[0;32m      3\u001b[0m     txt \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z0-9\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,txt)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     step 2\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(txt\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# step 3 and 5\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     stemming \u001b[38;5;241m=\u001b[39m [ps\u001b[38;5;241m.\u001b[39mstem(w) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\priet/nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\priet\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "job_df['Job.Description'] = job_df['Job.Description'].astype(str).apply(lambda x: cleaning(x))\n",
    "job_df['Title'] = job_df['Title'].astype(str).apply(lambda x: cleaning(x))\n",
    "job_df['Position'] = job_df['Position'].astype(str).apply(lambda x: cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ef18c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df['clean_text'] = job_df['Job.Description']+\" \"+job_df['Title']+job_df['Position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec674b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Job Summary Knowledge Universe (KU) Site Directors are site leaders who inspire children and teachers alike to learn and grow. They are passionate about educational excellence, and confident teaching children and adults. They use our nationally recognized curriculum as a framework to create unique and engaging classroom experiences. They are committed to making their site successful and know that meaningful relationships with children, families, and their team are important to success. They are fully engaged, enthusiastic about their work, and eager to share their knowledge with others.  Job Responsibilities and Essential Functions These are the basic expectations for Site Directors. Of course, creative and new ways to meet or exceed expectations are encouraged, so long as the required essential functions are also met. â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 supervision of children and staff  â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 record keeping  â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 licensing records and child files  â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lesson planning and implementation  â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 maintenance of safe and welcoming classroom environment  â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 building of relationships with the community and school  â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 recruiting new students to the program  â€¢\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 applicant must have strong organizational skills  Site Director @ Knowledge UniverseSite Director'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_df['clean_text'][64119]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d9bec7",
   "metadata": {},
   "source": [
    "# vectorizatoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bd5de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d55d267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "matrix = tfidf.fit_transform(job_df['clean_text'])\n",
    "similarity = cosine_similarity(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7d109c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.04316713, 0.01584323, ..., 0.03422459, 0.02355197,\n",
       "        0.05787057],\n",
       "       [0.04316713, 1.        , 0.02349895, ..., 0.02843237, 0.00650416,\n",
       "        0.02200366],\n",
       "       [0.01584323, 0.02349895, 1.        , ..., 0.12147903, 0.11821837,\n",
       "        0.0849148 ],\n",
       "       ...,\n",
       "       [0.03422459, 0.02843237, 0.12147903, ..., 1.        , 0.10231673,\n",
       "        0.0942541 ],\n",
       "       [0.02355197, 0.00650416, 0.11821837, ..., 0.10231673, 1.        ,\n",
       "        0.3645772 ],\n",
       "       [0.05787057, 0.02200366, 0.0849148 , ..., 0.0942541 , 0.3645772 ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdfd9a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(276, 0.9722978191534403),\n",
       " (730, 0.46934556239894065),\n",
       " (81, 0.45367410518365264),\n",
       " (917, 0.45367410518365264),\n",
       " (252, 0.23216213857943935),\n",
       " (128, 0.2103412366151445),\n",
       " (629, 0.16075217918132761),\n",
       " (825, 0.15085839382029045),\n",
       " (360, 0.13948993213260905),\n",
       " (38, 0.1320341335055304),\n",
       " (245, 0.12456254264617571),\n",
       " (326, 0.1212848168047901),\n",
       " (298, 0.11780183176064217),\n",
       " (195, 0.11298471725820095),\n",
       " (284, 0.11217975216761909),\n",
       " (59, 0.11167276537417432),\n",
       " (114, 0.10797827058611764),\n",
       " (254, 0.10562131035812325),\n",
       " (890, 0.10498251733474127)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(enumerate(similarity[0])), key=lambda x: x[1], reverse=True)[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d126d7",
   "metadata": {},
   "source": [
    "# Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a3e6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(title):\n",
    "\n",
    "        indx = job_df[job_df['Title'] == title].index[0]\n",
    "        indx = job_df.index.get_loc(indx)\n",
    "        distances = sorted(list(enumerate(similarity[indx])), key=lambda x: x[1], reverse=True)[1:20]\n",
    "\n",
    "        jobs = []\n",
    "        for i in distances:\n",
    "            jobs.append(job_df.iloc[i[0]].Title)\n",
    "        return jobs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bcb9affb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site director knowledg univers educ llc',\n",
       " 'teacher knowledg univers',\n",
       " 'assist teacher knowledg univers',\n",
       " 'assist teacher knowledg univers',\n",
       " 'cook knowledg univers',\n",
       " 'immedi open assist teacher la petit academi',\n",
       " 'summer school age assist children courtyard',\n",
       " 'hvac instructor vatterott educ center',\n",
       " 'temporari coordin site oper macyscom maci',\n",
       " 'fellowship program coordin connecticut children medic center',\n",
       " 'medic offic assist instructor concord career colleg inc',\n",
       " 'youth camp residenti assist activ coordin new york ny el educ servic',\n",
       " 'pharmaci adjunct instructor brown macki colleg',\n",
       " 'secur offic regular securita usa',\n",
       " 'secur offic 100000 job coalit securita usa',\n",
       " 'school day camp counselor ii ymca greenvil',\n",
       " 'faculti call zenith educ group',\n",
       " 'faculti support specialist zenith educ group',\n",
       " 'handbag sell specialist part time bloomingdal chevi chase md bloomingdal']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend('site director knowledg univers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e5e5470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(job_df,open('df.pkl','wb'))\n",
    "pickle.dump(similarity,open('similarity.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "239dc57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bae56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
